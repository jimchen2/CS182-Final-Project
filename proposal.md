## Project Overview
- **Objective**: Fine-tune a GPT model for a chatbox simulating a targeted person's style with background input.
- **Base Model**: GPT-3.5.
- **Finetuning Technique**: [LoRA/Soft-Prompting/Other].

## Participants
- **Name1**: [Jiamu Chen](https://jimchen.me) SID: 3038736105
- **Name2**: Xavier Chen SID: 3039771568
- **Name3**: Jackson Wei SID: 3036539208
- **Name4**: Stefan Pham SID: 3036694314

## High-Level Task Description
Vector combinations in Transformer has shown that Large Language Models(LLMs) have the potential to struct our language into a form of mathematics expressions, and is able to truly understand parts of our speech.
This task is trying to perform a higher level prompt, by introducting background information to GPT to see whether the model would change its style as we participated. If successful, the next step is to customize a chatbot with desired characteristics. Proof of this is by investing a real world celebrity, providing these information to GPT, and see if the language model has similar reflection as the real human.

## Data and Resources
- **Datasets**: Self-composed
- **Compute**: tenth of minutes
- **Storage**: Enough

## Milestones
- **Week 1**: Figure out the API of GPT fine tuning.
- **Week 2**: Compose designed datasets for fine tuning.
- **Week 3**: Analyze results and compare effects in different periods within fine tuning.
- **Week 4**: Summary and report.

## Expected Deliverables
- Fine-tuned model
- Generated text samples
- Performance analysis report
- Training / Testing loss curve


## References
- https://platform.openai.com/docs/guides/fine-tuning
- https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates

## GitHub Repo
https://github.com/jimchen2/CS182-Final-Project
