{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_article_links(file_path, output_file_path):\n",
    "    # Read the HTML content from the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    # Parsing the HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extracting all links starting with '/philosophy/'\n",
    "    articles = soup.find_all('a', href=lambda href: href and href.startswith('/philosophy/'))\n",
    "\n",
    "    # Base URL\n",
    "    base_url = \"https://www.gnu.org\"\n",
    "\n",
    "    # Constructing full URLs\n",
    "    full_urls = [base_url + article['href'] for article in articles]\n",
    "\n",
    "    # Write the URLs to the output file\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        for url in full_urls:\n",
    "            file.write(url + '\\n')\n",
    "\n",
    "# Example usage\n",
    "input_file_path = './gnu-source.html' # Replace this with the path to your HTML file\n",
    "output_file_path = './extracted_urls.txt' # The file where URLs will be saved\n",
    "extract_article_links(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re  # Importing the regular expressions module\n",
    "\n",
    "def extract_text_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        main_content = soup.find('div', {'id': 'content'})  # Adjust this selector based on the common container\n",
    "        if main_content:\n",
    "            text = ' '.join([p.get_text().strip() for p in main_content.find_all('p')])\n",
    "            # Replace multiple newlines or spaces with a single space\n",
    "            cleaned_text = re.sub(r'\\s+', ' ', text)\n",
    "            return cleaned_text\n",
    "        else:\n",
    "            return \"Main content not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def read_urls_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read().splitlines()\n",
    "\n",
    "def save_text_to_file(folder, filename, text):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    with open(os.path.join(folder, filename), 'w') as file:\n",
    "        file.write(text)\n",
    "\n",
    "# Main process\n",
    "urls_file_path = './extracted_urls.txt'  # Change this to your file path\n",
    "urls = read_urls_from_file(urls_file_path)\n",
    "articles_folder = 'articles'\n",
    "\n",
    "for url in urls:\n",
    "    content = extract_text_from_url(url)\n",
    "    file_name = url.split('/')[-1].split('?')[0] + '.txt'  # Creating a filename from the URL\n",
    "    save_text_to_file(articles_folder, file_name, content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "\n",
    "def extract_text_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        main_content = soup.find('div', {'id': 'content'})\n",
    "        if main_content:\n",
    "            # Extracting text and formatting as Markdown\n",
    "            markdown_text = '\\n\\n'.join(['**' + p.get_text().strip() + '**' if p.find('strong') else p.get_text().strip() for p in main_content.find_all('p')])\n",
    "            # Replace multiple newlines or spaces with a single space\n",
    "            cleaned_text = re.sub(r'\\s+', ' ', markdown_text)\n",
    "            return cleaned_text\n",
    "        else:\n",
    "            return \"Main content not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def read_urls_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read().splitlines()\n",
    "\n",
    "def save_text_to_file(folder, filename, text):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    with open(os.path.join(folder, filename + '.md'), 'w') as file:  # Save as Markdown file\n",
    "        file.write(text)\n",
    "\n",
    "# Main process\n",
    "urls_file_path = './extracted_urls.txt'\n",
    "urls = read_urls_from_file(urls_file_path)\n",
    "articles_folder = 'articles'\n",
    "\n",
    "for url in urls:\n",
    "    content = extract_text_from_url(url)\n",
    "    file_name = url.split('/')[-1].split('?')[0]  # Creating a filename from the URL\n",
    "    save_text_to_file(articles_folder, file_name, content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
